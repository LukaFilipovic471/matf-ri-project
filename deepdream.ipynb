{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a353f564-9d55-49c1-af93-5039363fe54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92dbf223-3ec4-4071-b930-5fed8a454832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes=1000, aux_logits=False, transform_input=False):\n",
    "        super(InceptionV3, self).__init__()\n",
    "        self.inception = models.inception_v3(pretrained=False, num_classes=num_classes, aux_logits=aux_logits, transform_input=transform_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inception(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494e3759-edfa-4fc4-9782-e0068e8e9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DeepDream model\n",
    "class DeepDream(nn.Module):\n",
    "    def __init__(self, model, layer_idx):\n",
    "        super(DeepDream, self).__init__()\n",
    "        self.features = self.get_required_layers(model, layer_idx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    def get_required_layers(self, model, layer_idx):\n",
    "        # Extract the desired layers from the InceptionV3 model\n",
    "        if isinstance(model, InceptionV3):\n",
    "            # InceptionV3 has multiple branches, so we need to extract from a specific submodule\n",
    "            return nn.Sequential(*list(model.inception.children())[:layer_idx+1])\n",
    "        elif isinstance(model, nn.Module):\n",
    "            return model  # For simplicity, using the entire model\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e945b49-a78f-456c-8687-2bd345e4f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dream(image_tensor, model, layer_idx, iterations, lr, octave_scale, output_path):\n",
    "    # Convert image tensor to nn.Parameter\n",
    "    img = nn.Parameter(image_tensor.to(device))\n",
    "\n",
    "    # Define the deep dream model\n",
    "    dream_model = DeepDream(model, layer_idx).to(device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam([img], lr=lr)\n",
    "\n",
    "    # DeepDream iterations\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        features = dream_model(img)\n",
    "        loss = features.norm()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Apply the octave scaling\n",
    "        img.data = img.data + octave_scale * img.grad.data\n",
    "\n",
    "        # Zero the gradient\n",
    "        img.grad.data.zero_()\n",
    "\n",
    "        # Clip the image values to be in the valid range\n",
    "        img.data = torch.clamp(img.data, 0, 1)\n",
    "\n",
    "    # Save the final deep dream image\n",
    "    result = transforms.ToPILImage()(img.squeeze(0).cpu())\n",
    "    result.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a44137-1998-4a8b-a44b-b2a28ff317a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the layer index for deep dream (you can experiment with different layers)\n",
    "layer_index = 10\n",
    "\n",
    "iterations = 20\n",
    "learning_rate = 0.01\n",
    "octave_scale = 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b494bc29-6234-4092-813a-92d5bda68358",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to the same dimensions as expected by InceptionV3\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to the same dimensions as expected by InceptionV3\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a6aba3-28c0-4c34-8c5a-b7abe0ce3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/user/.local/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = InceptionV3().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf1ee05-7512-4c2e-97e3-f30b0ad6520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root='./data/train', transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root='./data/validation', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ba095b-523a-4b40-ae72-ec7edba7bfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_dataset[10]\n",
    "transform = transforms.ToPILImage()\n",
    "#img = transform(img)\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e906867a-7e55-47d4-aefc-072c33e0e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_dream(img.reshape(1, img.shape[0], img.shape[1], img.shape[2]), model, layer_index, iterations, learning_rate, octave_scale, output_path='outputImg.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90bf0df7-a5d0-4047-bae2-25f88551e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2000):\n",
    "#     img, label = train_dataset[i]\n",
    "#     output_image_path = f'deepdream_train_pic{i}.jpg'\n",
    "#     deep_dream(img.reshape(1, img.shape[0], img.shape[1], img.shape[2]), model, layer_index, iterations, learning_rate, octave_scale, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e73d962-078d-4708-b9ab-5c8e8f998a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     img, label = test_dataset[i]\n",
    "#     output_image_path = f'deepdream_test_pic{i}.jpg'\n",
    "#     deep_dream(img.reshape(1, img.shape[0], img.shape[1], img.shape[2]), model, layer_index, iterations, learning_rate, octave_scale, output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53a73f7-5b01-422a-871e-4566698a1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_deepdream = datasets.ImageFolder(root='./data_deepdream/train', transform=train_transform)\n",
    "test_dataset_deepdream = datasets.ImageFolder(root='./data_deepdream/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7d4367-b2bf-4ab8-9210-ed6a93d0b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, label = train_dataset_deepdream[1]\n",
    "# transform = transforms.ToPILImage()\n",
    "# img = transform(img)\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e941290b-f090-41da-be75-62bfa436129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset_deepdream, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_deepdream, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7697ca1c-15fa-44d2-b1cb-a1cbe73e8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bdd54c6-5e49-4c5b-8787-9a373bf33c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        num_same = 0\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if len(outputs.shape) == 1:\n",
    "                preds = outputs > 0\n",
    "                num_same += sum(preds == y).item()\n",
    "            else:\n",
    "                _, indices = torch.max(outputs, 1)\n",
    "                num_same += sum(indices == y).item()\n",
    "        print(f'Average loss: {total_loss / len(dataloader.dataset)}')\n",
    "        print(f'Accuracy: {num_same / len(dataloader.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9f6d2c-3211-442f-917f-29f034ed641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.02889556265559908\n",
      "Accuracy: 0.5541922290388548\n",
      "Average loss: 0.025065351056662563\n",
      "Accuracy: 0.5439672801635992\n",
      "Average loss: 0.021226532863936786\n",
      "Accuracy: 0.5838445807770961\n",
      "Average loss: 0.03665846056002049\n",
      "Accuracy: 0.5971370143149284\n",
      "Average loss: 0.027172279199452\n",
      "Accuracy: 0.6513292433537833\n",
      "Average loss: 0.02852359042333435\n",
      "Accuracy: 0.549079754601227\n",
      "Average loss: 0.10909560371517892\n",
      "Accuracy: 0.5623721881390593\n",
      "Average loss: 0.021737083122286574\n",
      "Accuracy: 0.6359918200408998\n",
      "Average loss: 0.03531157293934032\n",
      "Accuracy: 0.6165644171779141\n",
      "Average loss: 0.024312344492091235\n",
      "Accuracy: 0.6830265848670757\n",
      "Average loss: 0.021233350769873777\n",
      "Accuracy: 0.656441717791411\n",
      "Average loss: 0.02237178254225015\n",
      "Accuracy: 0.6646216768916156\n",
      "Average loss: 0.02412903665399259\n",
      "Accuracy: 0.6390593047034765\n",
      "Average loss: 0.06951042077292693\n",
      "Accuracy: 0.5725971370143149\n",
      "Average loss: 0.022285732815845855\n",
      "Accuracy: 0.6226993865030674\n",
      "Average loss: 0.03785216302471902\n",
      "Accuracy: 0.5725971370143149\n",
      "Average loss: 0.06797664735946188\n",
      "Accuracy: 0.588957055214724\n",
      "Average loss: 0.030760289518379728\n",
      "Accuracy: 0.623721881390593\n",
      "Average loss: 0.042809463290836666\n",
      "Accuracy: 0.591002044989775\n",
      "Average loss: 0.03387484966124006\n",
      "Accuracy: 0.6390593047034765\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.to(device)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    test_loop(test_dataloader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c81e9a-0f62-4a97-a5c9-e108f855cefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
